{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\anaconda3\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# import standard libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel, cosine_similarity\n",
    "import time\n",
    "from gensim.models import LdaMulticore, LdaModel\n",
    "from gensim.models import Phrases\n",
    "from gensim.models.phrases import Phraser\n",
    "from gensim import corpora\n",
    "from gensim.models import TfidfModel\n",
    "import pickle\n",
    "import wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File data/combined_df.csv does not exist: 'data/combined_df.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-cfbd28877737>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'data/combined_df.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'lemmatized'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'lemmatized'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m','\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    674\u001b[0m         )\n\u001b[0;32m    675\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 676\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    446\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    447\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 448\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    449\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    450\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    879\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 880\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    881\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    882\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1112\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1113\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1114\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1115\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1116\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1889\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1890\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1891\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1892\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1893\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] File data/combined_df.csv does not exist: 'data/combined_df.csv'"
     ]
    }
   ],
   "source": [
    "df=pd.read_csv('data/combined_df.csv')\n",
    "df['lemmatized']=df['lemmatized'].apply(lambda x:x.split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_dist = open(\"data/dict.pickle\",\"rb\")\n",
    "doc_topic_dist = pickle.load(dict_dist)\n",
    "cosine_pickle = open(\"data/cosine.pickle\",\"rb\")\n",
    "cosine_sim_all=pickle.load(cosine_pickle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LdaModel.load('LDA/lda_combined_review')\n",
    "corpus_pickle = open(\"LDA/corpus.pickle\",\"rb\")\n",
    "corpus=pickle.load(corpus_pickle)\n",
    "dictionary_pickle = open(\"LDA/dictionary.pickle\",\"rb\")\n",
    "dictionary=pickle.load(dictionary_pickle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the weighted rating based on IMDB formula\n",
    "def weighted_rating(x, m, C):\n",
    "    v = x['audience_count']\n",
    "    R = x['audience_rating']\n",
    "    return (v/(v+m) * R) + (m/(m+v) * C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference: https://www.kaggle.com/ktattan/lda-and-document-similarity/data\n",
    "from scipy.spatial import distance\n",
    "def jensen_shannon(query, matrix):\n",
    "    \"\"\"\n",
    "    This function implements a Jensen-Shannon similarity\n",
    "    between the input query (an LDA topic distribution for a document)\n",
    "    and the entire corpus of topic distributions.\n",
    "    It returns an array of length M where M is the number of documents in the corpus\n",
    "    \"\"\"\n",
    "    sim = [distance.jensenshannon(data,query) for data in matrix]\n",
    "    return sim\n",
    "\n",
    "def get_most_similar_documents(query,matrix,k=10):\n",
    "    \"\"\"\n",
    "    This function implements the Jensen-Shannon distance above\n",
    "    and retruns the top k indices of the smallest jensen shannon distances\n",
    "    \"\"\"\n",
    "    sim = jensen_shannon(query,matrix) # list of jensen shannon distances\n",
    "\n",
    "    return np.argsort(sim)[:k] # the top k positional index of the smallest Jensen Shannon distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_score_rating(target_idx, movie_indices, sim_scores):\n",
    "    \n",
    "    movie_content_rating=df.iloc[target_idx]['content_rating']  \n",
    "    movie_year=df.iloc[target_idx]['year']\n",
    "    \n",
    "    selected_movies=df.iloc[movie_indices]\n",
    "    audience_counts = selected_movies[selected_movies['audience_count'].notnull()]['audience_count'].astype('int')\n",
    "    \n",
    "    m = audience_counts.quantile(0.5)\n",
    "\n",
    "    C=selected_movies['audience_rating'].mean()\n",
    "    wr=selected_movies.apply(lambda x: weighted_rating(x,m,C), axis=1)\n",
    "\n",
    "    selected_df=pd.DataFrame(df[['movie_title','content_rating','year','audience_rating','audience_count']].iloc[movie_indices])\n",
    "    selected_df['Score']=[x[1] for x in sim_scores]\n",
    "    selected_df['wr']=wr\n",
    "    #Product for similarity and rating so priortise to recommend high similarity and high rating movies\n",
    "    \n",
    "    selected_df['mix_score']=selected_df['wr']*selected_df['Score']\n",
    "    \n",
    "    \n",
    "    #Set the limit for number of audience count\n",
    "    #At least higher than 50% quantile of the audience count in the whole document\n",
    "    count_bound=df['audience_count'].quantile(0.5)\n",
    "    #Remove movie that is over the target movie content rating and with too few audience\n",
    "    #ranked by the mix score\n",
    "    #Only select movie that is too old compared with the target movie, threshold is a decade-10 years.\n",
    "    selected_df=selected_df[(selected_df['content_rating']<=movie_content_rating) & (selected_df['audience_count']>count_bound) & (selected_df['year']>movie_year-10)]\n",
    "    selected_df=selected_df.sort_values('mix_score', ascending=False)\n",
    "\n",
    "    # Return the top 10 most similar movies\n",
    "    return selected_df[['movie_title','year','Score','audience_rating', 'audience_count']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend2(title, cosine_sim,weight=0.5, n=6):\n",
    "    indices = pd.Series(df.index, index=df['movie_title']).drop_duplicates()\n",
    "    movie_list = df[df['movie_title'].str.contains(title)]\n",
    "    if len(movie_list):\n",
    "        #In case of similar movie title like 'Iron Man' and 'Iron Man 2'\n",
    "        if any(movie_list['movie_title']==title):\n",
    "            movie_title=title\n",
    "        else:\n",
    "           # Pick the one with the highest audience rating\n",
    "            movie_title=movie_list.sort_values(by=['audience_rating'], ascending=False)['movie_title'].iloc[0]\n",
    "        \n",
    "        print('Selected movie:',movie_title)\n",
    "    \n",
    "        #Some movies are duplicated such as Frozen has two version.\n",
    "        #Pick the one with higher audience rating\n",
    "        idx = indices[movie_title]\n",
    "        if np.isscalar(idx)==False:\n",
    "            idx=df.iloc[idx].sort_values(by=['audience_rating'], ascending=False).index[0]\n",
    "\n",
    "          \n",
    "        \n",
    "        test_bow = dictionary.doc2bow(df.loc[idx]['lemmatized'])\n",
    "\n",
    "        test_doc_distribution = np.array([tup[1] for tup in lda.get_document_topics(bow=test_bow)])\n",
    "        lda_score=jensen_shannon(test_doc_distribution,doc_topic_dist)\n",
    "        lda_score_inv=[1-x for x in lda_score]\n",
    "\n",
    "        weight_score=zip([weight*x for x in lda_score_inv],[(1-weight)*y for y in cosine_sim[idx]])\n",
    "        weighted_score=[x+y for x,y in weight_score]\n",
    "        weight_rank=sorted(list(enumerate(weighted_score)), key=lambda x:x[1], reverse=True)[1:]\n",
    "\n",
    "        weight_movie_indices = [i[0] for i in weight_rank]\n",
    "        \n",
    "        #Created another function for simplification \n",
    "        #Adding constraints to the  movie recommendation list\n",
    "        recommend=combine_score_rating(idx,weight_movie_indices, weight_rank)\n",
    "        \n",
    "        #recommend=df_new.iloc[weight_movie_indices]\n",
    "        #return df_new.iloc[weight_movie_indices[0:10]][['movie_title','actors','audience_rating']]\n",
    "        print('=====================')\n",
    "        for i in range(n):\n",
    "            recommend_movie=recommend.iloc[i]['movie_title']\n",
    "            movie_year=recommend.iloc[i]['year']\n",
    "            print(f\"{i+1}. {recommend_movie}, {movie_year}\")\n",
    "        #return recommend.head(n)\n",
    "    else:\n",
    "        print('No records in our database. Please check your input')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected movie: Inception\n",
      "=====================\n",
      "1. Interstellar, 2014\n",
      "2. Source Code, 2011\n",
      "3. Catch Me If You Can, 2002\n",
      "4. Avatar, 2009\n",
      "5. Gravity, 2013\n",
      "6. Minority Report, 2002\n"
     ]
    }
   ],
   "source": [
    "recommend2('Inception', cosine_sim_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batchcomplete': True,\n",
       " 'query': {'pages': [{'pageid': 23270459,\n",
       "    'ns': 0,\n",
       "    'title': 'Inception',\n",
       "    'terms': {'label': ['Inception'],\n",
       "     'description': ['2010 science fiction film directed by Christopher Nolan']}}]}}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "def get_wiki_main_image(title):\n",
    "    url = 'https://en.wikipedia.org/w/api.php'\n",
    "    data = {\n",
    "        'action' :'query',\n",
    "        'format' : 'json',\n",
    "        'formatversion' : 2,\n",
    "        'prop' : 'pageimages|pageterms',\n",
    "        'piprop' : 'original',\n",
    "        'titles' : title\n",
    "    }\n",
    "    response = requests.get(url, data)\n",
    "    json_data = json.loads(response.text)\n",
    "    #return json_data['query']['pages'][0]['source'] if len(json_data['query']['pages']) >0 else 'Not found'\n",
    "    return json_data\n",
    "get_wiki_main_image('Inception')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://upload.wikimedia.org/wikipedia/commons/3/3e/Emma_Thomas_%26_Christopher_Nolan_at_WonderCon_2010_1.JPG',\n",
       " 'https://upload.wikimedia.org/wikipedia/commons/3/34/Impossible_staircase.svg',\n",
       " 'https://upload.wikimedia.org/wikipedia/commons/4/43/InceptionCastPremiereJuly10.jpg',\n",
       " 'https://upload.wikimedia.org/wikipedia/commons/f/fa/Wikiquote-logo.svg',\n",
       " 'https://upload.wikimedia.org/wikipedia/en/4/4a/Commons-logo.svg',\n",
       " 'https://upload.wikimedia.org/wikipedia/en/2/2e/Inception_%282010%29_theatrical_poster.jpg',\n",
       " 'https://upload.wikimedia.org/wikipedia/en/8/8a/OOjs_UI_icon_edit-ltr-progressive.svg',\n",
       " 'https://upload.wikimedia.org/wikipedia/en/1/1b/Semi-protection-shackle.svg',\n",
       " 'https://upload.wikimedia.org/wikipedia/en/9/96/Symbol_category_class.svg',\n",
       " 'https://upload.wikimedia.org/wikipedia/en/9/94/Symbol_support_vote.svg',\n",
       " 'https://upload.wikimedia.org/wikipedia/en/e/e7/Video-x-generic.svg']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikipedia.page(\"Inception movie\").images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
